{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# This jupyter notebook was done as a refresher to using Multi-Layer Perceptron neural network from Scikit-learn. This notebook also has notes on applying grid search cross-validation on the MLP classifier.\n",
    "## The dataset used is the UCI heart disease from kaggle.com\n",
    "## Note that the results in this notebook are not optimal, but this notebook was done as practice using the classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# importing important libraries\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reading the heart disease csv file\n",
    "heart_df = pd.read_csv(r\"C:\\Users\\KennoHead\\Desktop\\Data Science and Machine Learning Refresher\\heart.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# setting up the training and testing sets\n",
    "\n",
    "heart_features_cols = list(heart_df.columns)\n",
    "heart_features_cols.remove('target')\n",
    "\n",
    "heart_features = heart_df[heart_features_cols]\n",
    "heart_labels = heart_df['target']\n",
    "\n",
    "features_training, features_testing, labels_training, labels_testing = train_test_split(heart_features,\n",
    "                                                            heart_labels, test_size = 0.3, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# notes\n",
    "# hidden layer sizes: the ith element is the number of neurons for that ith layer\n",
    "# in this case, 1st layers have 48 neurons, the 2nd layer has 24 neurons, \n",
    "# the 3rd hash 12 neurons, and the 4th has 6\n",
    "\n",
    "# activation: the activation function should be non linear, logistic refers to the sigmoid function\n",
    "# similar to the function used in logistic regression\n",
    "# solver: the kind of minimization problem, in this case 'adam' refers to stochastic gradient descent\n",
    "# alpha: the regularization term, or the penalty term to add into the error function\n",
    "# learning rate init: the learning rate\n",
    "# verbose : boolean whether to print progress or not\n",
    "# tol: tolerance for the optimization, when the loss/score is not improving by at least the tol value\n",
    "# in consecutive iterations\n",
    "\n",
    "a_MLP_ANN = MLPClassifier(hidden_layer_sizes = (48, 24, 12, 6 ), activation = 'logistic',\n",
    "                         solver = 'adam', alpha = 1e-5, random_state = 0,\n",
    "                         learning_rate_init = 0.1, verbose = True, tol = 1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69796975\n",
      "Iteration 2, loss = 0.69378513\n",
      "Iteration 3, loss = 0.70869283\n",
      "Iteration 4, loss = 0.72662323\n",
      "Iteration 5, loss = 0.70431848\n",
      "Iteration 6, loss = 0.69034534\n",
      "Iteration 7, loss = 0.68824715\n",
      "Iteration 8, loss = 0.68688709\n",
      "Iteration 9, loss = 0.68757769\n",
      "Iteration 10, loss = 0.68773387\n",
      "Iteration 11, loss = 0.68643943\n",
      "Iteration 12, loss = 0.69038136\n",
      "Iteration 13, loss = 0.69605791\n",
      "Iteration 14, loss = 0.69865772\n",
      "Iteration 15, loss = 0.69492893\n",
      "Iteration 16, loss = 0.68885509\n",
      "Iteration 17, loss = 0.68750973\n",
      "Iteration 18, loss = 0.68696555\n",
      "Iteration 19, loss = 0.68708253\n",
      "Iteration 20, loss = 0.68740208\n",
      "Iteration 21, loss = 0.68891778\n",
      "Iteration 22, loss = 0.68871070\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=1e-05,\n",
       "              hidden_layer_sizes=(48, 24, 12, 6), learning_rate_init=0.1,\n",
       "              random_state=0, tol=1e-06, verbose=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# to train the MLP ANN\n",
    "a_MLP_ANN.fit(features_training, labels_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5164835164835165\n",
      "[[0.46199918 0.53800082]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.46199919 0.53800081]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.46199918 0.53800082]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]\n",
      " [0.4619992  0.5380008 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# a_MLP_ANN.coefs_, use this to get the weight matrix corresponding to the layer i.\n",
    "# a_MLP_ANN.intercepts_, use this to get the bias vector for the ith layer\n",
    "\n",
    "# .predict to predict the labels of the test features\n",
    "MLP_ANN_predictions = a_MLP_ANN.predict(features_testing)\n",
    "\n",
    "MLP_ANN_acc = accuracy_score(labels_testing, MLP_ANN_predictions)\n",
    "\n",
    "print(MLP_ANN_acc)\n",
    "\n",
    "# .predict_proba to get probabilities of each class/label\n",
    "MLP_ANN_probabilities = a_MLP_ANN.predict_proba(features_testing)\n",
    "\n",
    "print (MLP_ANN_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 44]\n",
      " [ 0 47]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# using confusion matrices\n",
    "# note that with 2 labels, there should be 2 classes\n",
    "# so the matrix will be 2x2 in dimensions\n",
    "\n",
    "Con_Matrix = metrics.confusion_matrix(labels_testing, MLP_ANN_predictions)\n",
    "\n",
    "print(Con_Matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44825918762088973\n"
     ]
    }
   ],
   "source": [
    "\n",
    "FPR, TPR, thresholds = metrics.roc_curve(labels_testing, MLP_ANN_probabilities[:,1], pos_label = 1 )\n",
    "\n",
    "MLP_ANN_AUC = metrics.auc(FPR, TPR)\n",
    "\n",
    "print(MLP_ANN_AUC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
